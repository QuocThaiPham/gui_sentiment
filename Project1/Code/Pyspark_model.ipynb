{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee479f2-28bc-4da7-8f21-be66cae0fdc4",
   "metadata": {
    "id": "lTSukfVFoM-4"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0316e6f8-9179-4eef-bc1d-41f9776685b6",
   "metadata": {
    "id": "FPAqiuGwmqmA"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Build feature\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer,StringIndexer,OneHotEncoder\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF, Tokenizer\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# ML algrothism\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier, NaiveBayes, NaiveBayesModel, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# other\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20b1fc48-6c97-4c0b-97e0-487c62b6fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(predicted_df, num_class=3, label='label'):\n",
    "    #calculate confusion matrix\n",
    "    import numpy as np\n",
    "    _df = predicted_df.groupBy(label, 'prediction').count()\n",
    "    confusion_matrix = np.array([])\n",
    "    for i in range(3):\n",
    "        for j in range(num_class):\n",
    "            try: \n",
    "                x = _df.select('count').where((_df[label]==i) & (_df['prediction']==j)).collect()[0][0]\n",
    "                confusion_matrix = np.append(confusion_matrix, x)\n",
    "            except IndexError as e:\n",
    "                confusion_matrix = np.append(confusion_matrix, 0)\n",
    "    return confusion_matrix.reshape(num_class,num_class).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9305e361-9bea-46a6-8c86-6fe28b6e0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(confusion_matrix, num_class=2):\n",
    "    import numpy as np\n",
    "    #def for calculate accuracy, recall, precision from confusion matrix\n",
    "    cnf_sum = np.sum(confusion_matrix)\n",
    "    sum_ver = np.sum(confusion_matrix, axis=0)\n",
    "    sum_hor = np.sum(confusion_matrix, axis=1)\n",
    "    recall = np.array([])\n",
    "    preci = np.array([])\n",
    "    acc = (confusion_matrix[0,0]+confusion_matrix[1,1])/cnf_sum\n",
    "    for i in range(num_class):\n",
    "        #recall\n",
    "        _recall = np.array(confusion_matrix[i,i]/sum_hor[i])\n",
    "        recall = np.append(recall, _recall)\n",
    "        #precision\n",
    "        _preci = np.array(confusion_matrix[i,i]/sum_ver[i])\n",
    "        preci = np.append(preci, _preci)\n",
    "        #f1-score\n",
    "        f1 = 2/((1/recall)+(1/preci))\n",
    "    return {'accuracy': acc, 'recall':recall,'precision': preci, 'f1':f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fbacc0e-f9ce-43fb-89f5-f0c53b7ced73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_matrix, normalize=False):\n",
    "    # Visualization confusion matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    \n",
    "    sns.heatmap(confusion_matrix, square=True ,annot=True, fmt='.0f', cbar=False,\n",
    "                xticklabels=[0, 1], yticklabels=[0, 1], cmap=plt.cm.GnBu)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('Actual Label')\n",
    "    \n",
    "    if normalize:\n",
    "        plt.subplot(1,2,2)\n",
    "        confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1, keepdims = True)\n",
    "        sns.heatmap(confusion_matrix, square=True ,annot=True, fmt='.4f', cbar=False,\n",
    "                xticklabels=[0, 1], yticklabels=[0, 1], cmap=plt.cm.GnBu)\n",
    "        plt.title('Confusion Matrix with normalize')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('Actual Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2541f4-41b6-4606-93e3-79880f7d9ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '16g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "868141e2-312d-4007-8313-2754c6a0ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty(key='spark.hadoop.dfs.client.use.datanode.hostname',value='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf87aec-d8a4-4327-aa8d-6a3caad6eb04",
   "metadata": {
    "id": "wnB3qrDkAJNX"
   },
   "outputs": [],
   "source": [
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cca893c-18bd-442a-b152-ad3e8a1dc47b",
   "metadata": {
    "id": "2mfBvmJoejHV"
   },
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6e313d4-4371-4928-bdcf-12a5f61373bc",
   "metadata": {
    "id": "bxVoyCsJAU1Q"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b7f79-ef53-42e7-b506-c244a24748e3",
   "metadata": {},
   "source": [
    "# About Dataset\n",
    "## Acknowledgements\n",
    "Acccording to data about review, build a model to determine this review is positive or negative or netral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31c66a7c-b64e-4225-8bda-755c7bd901a0",
   "metadata": {
    "id": "49f4RDelbtJq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------+---------+---------+----------+--------+\n",
      "| ID|          Restaurant|             Address|district|price_max|price_min|start_time|end_time|\n",
      "+---+--------------------+--------------------+--------+---------+---------+----------+--------+\n",
      "|  1|Gà Rán & Burger M...|2 - 6 Bis Điện Bi...|       1|  50000.0| 200000.0|     00:00|   23:59|\n",
      "|  2|Cháo Trắng - Cháo...|112B Phạm Viết Ch...|       1|   5000.0|  40000.0|     00:00|   23:59|\n",
      "|  3|Texas Chicken - N...|115 Nguyễn Thái H...|       1|  30000.0| 300000.0|      null|    null|\n",
      "|  4|        Bếp Chay 365|Tầng 1, 35 Nguyễn...|       1|  15000.0|  50000.0|      null|    null|\n",
      "|  5|  Bánh Canh Cua Linh|80A Điện Biên Phủ...|       1|  25000.0|  35000.0|     06:20|   22:00|\n",
      "|  6|Bún Đậu Mạc Văn K...|90 Trần Quang Khả...|       1|  45000.0| 140000.0|      null|    null|\n",
      "|  7|Bún Riêu Cua Ốc P...|66 Nguyễn Thái Bì...|       1|  35000.0|  55000.0|     06:30|   21:30|\n",
      "|  8|Há Cảo Đặc Biệt N...|86 Nguyễn Du, Quậ...|       1|   5000.0|  20000.0|      null|    null|\n",
      "|  9|Gà Rán KFC - Nguy...|2 Nguyễn Huy Tự, ...|       1|  30000.0|  88000.0|      null|    null|\n",
      "| 10|Hey Pelo - Origin...|Số 60 Trần Khắc C...|       1|  30000.0| 110000.0|      null|    null|\n",
      "| 11|Home Mì - Mì Ý, S...|65A Cách Mạng Thá...|       1|  27000.0| 100000.0|      null|    null|\n",
      "| 12|Chè Ngon Cô Ba - ...|176 Cô Giang, P. ...|       1|  15000.0| 100000.0|      null|    null|\n",
      "| 13|Geylang By Night ...|25 Cô Bắc, P. Cầu...|       1|  10000.0| 100000.0|      null|    null|\n",
      "| 14|Hẻm Fast Food - M...|75 Nguyễn Cư Trin...|       1|  10000.0| 100000.0|      null|    null|\n",
      "| 15|HỦ TÍU NAM VANG D...|60 Tôn Thất Đạm, ...|       1|  10000.0| 100000.0|      null|    null|\n",
      "| 16|Quán Nem - Nem Cu...|15E Nguyễn Thị Mi...|       1|  20000.0|  60000.0|      null|    null|\n",
      "| 17|Marukame Udon - U...|342 - 344 Hai Bà ...|       1|  60000.0| 300000.0|      null|    null|\n",
      "| 18|Xôi Gà Tân Định 3...|297 Hai Bà Trưng,...|       1|  15000.0|  25000.0|      null|    null|\n",
      "| 19|           Bò Bía Lý|120E Đinh Tiên Ho...|       1|   5000.0|   5000.0|      null|    null|\n",
      "| 20|Nem Nướng D'ran -...|17/7B Nguyễn Thị ...|       1|  40000.0|  70000.0|      null|    null|\n",
      "+---+--------------------+--------------------+--------+---------+---------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restaurant_data = spark.read.option(\"multiline\", \"true\").csv(\"../Clean_data/clean_restaurant.csv\",\n",
    "                                                             inferSchema=False,\n",
    "                                                             header=True)\n",
    "\n",
    "restaurant_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71f88b00-e392-4a57-9524-163e5018b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------+------------------+--------------------+----------------+-------------+-----+\n",
      "|IDRestaurant|          date_time|user_id|     rating_scaler|        clean_review|sentiment_encode|rating_encode|label|\n",
      "+------------+-------------------+-------+------------------+--------------------+----------------+-------------+-----+\n",
      "|           1|2023-12-20 21:13:00|   8670| 5.680000000000001|gà chiên còn sống...|             0.0|          2.0|  2.0|\n",
      "|           1|2023-09-25 15:43:00|  11063|               5.5|     đã ăn ăn đồng_ý|             0.0|          2.0|  2.0|\n",
      "|           1|2023-06-24 11:11:00|   9541|               5.5|     đã thư rất ngon|             1.0|          2.0|  2.0|\n",
      "|           1|2022-12-22 14:58:00|   9112|               1.9|về gói thêm khách...|             0.0|          0.0|  0.0|\n",
      "|           1|2022-09-23 22:40:00|   9651|              4.78|nhỏ kêu đói hồi b...|             0.0|          2.0|  2.0|\n",
      "|           1|2022-09-15 11:32:00|  11343|              7.66|có khuyến_mại tặn...|             1.0|          1.0|  1.0|\n",
      "|           1|2022-07-04 01:31:00|    376|               1.9|thề lâu lắm mới t...|             0.0|          0.0|  0.0|\n",
      "|           1|2022-06-25 20:13:00|  13768|              10.0|là lần đầu bạn đế...|             0.0|          1.0|  2.0|\n",
      "|           1|2022-06-10 12:43:00|   3643|               1.9|đặt kết_hợp thêm ...|             2.0|          0.0|  0.0|\n",
      "|           1|2022-03-19 13:48:00|    446|              6.04|hích phần cơm dở ...|             0.0|          2.0|  2.0|\n",
      "|           1|2022-01-26 22:31:00|   9552|               1.9|            dởm cười|             1.0|          0.0|  2.0|\n",
      "|           1|2021-06-10 22:45:00|  12566|              4.78|tới lúc bạn nhân_...|             2.0|          2.0|  2.0|\n",
      "|           1|2021-05-19 14:57:00|   1736|               1.9|sáng đã ghé đakao...|             2.0|          0.0|  0.0|\n",
      "|           1|2021-04-24 19:37:00|   6788|4.0600000000000005|thiếu gà nhờ hàng...|             0.0|          2.0|  2.0|\n",
      "|           1|2021-01-13 17:00:00|  11386|              8.02|đi bạn dùng tại_c...|             0.0|          1.0|  2.0|\n",
      "|           1|2021-01-02 22:42:00|  12116|               7.3|                null|             2.0|          1.0|  1.0|\n",
      "|           1|2020-12-14 22:16:00|  13146|               1.9|ăn tại_chỗ không_...|             0.0|          0.0|  0.0|\n",
      "|           1|2020-12-14 00:14:00|   6983|              7.12|càng ngày càng ch...|             0.0|          1.0|  2.0|\n",
      "|           1|2020-12-11 13:48:00|  13146|               1.9|đặt phần miếng ứn...|             1.0|          0.0|  2.0|\n",
      "|           1|2020-10-29 22:49:00|   6377|               1.9|drive xuyên nhớ k...|             2.0|          0.0|  0.0|\n",
      "+------------+-------------------+-------+------------------+--------------------+----------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_data = spark.read.option(\"multiline\", \"true\").csv(\"../Clean_data/clean_review_data.csv\",\n",
    "                      inferSchema=True,header=True)\n",
    "review_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae8290-08de-4f1a-a938-c9d5ea9c125b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acf9469c-1dc8-4233-8b95-e131d7f4aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------+-----------------+--------------------+----------------+-------------+-----+\n",
      "|IDRestaurant|          date_time|user_id|    rating_scaler|        clean_review|sentiment_encode|rating_encode|label|\n",
      "+------------+-------------------+-------+-----------------+--------------------+----------------+-------------+-----+\n",
      "|           1|2023-12-20 21:13:00|   8670|5.680000000000001|gà chiên còn sống...|             0.0|          2.0|  2.0|\n",
      "|           1|2023-09-25 15:43:00|  11063|              5.5|     đã ăn ăn đồng_ý|             0.0|          2.0|  2.0|\n",
      "|           1|2023-06-24 11:11:00|   9541|              5.5|     đã thư rất ngon|             1.0|          2.0|  2.0|\n",
      "|           1|2022-12-22 14:58:00|   9112|              1.9|về gói thêm khách...|             0.0|          0.0|  0.0|\n",
      "|           1|2022-09-23 22:40:00|   9651|             4.78|nhỏ kêu đói hồi b...|             0.0|          2.0|  2.0|\n",
      "|           1|2022-09-15 11:32:00|  11343|             7.66|có khuyến_mại tặn...|             1.0|          1.0|  1.0|\n",
      "|           1|2022-07-04 01:31:00|    376|              1.9|thề lâu lắm mới t...|             0.0|          0.0|  0.0|\n",
      "|           1|2022-06-25 20:13:00|  13768|             10.0|là lần đầu bạn đế...|             0.0|          1.0|  2.0|\n",
      "|           1|2022-06-10 12:43:00|   3643|              1.9|đặt kết_hợp thêm ...|             2.0|          0.0|  0.0|\n",
      "|           1|2022-03-19 13:48:00|    446|             6.04|hích phần cơm dở ...|             0.0|          2.0|  2.0|\n",
      "+------------+-------------------+-------+-----------------+--------------------+----------------+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- IDRestaurant: integer (nullable = true)\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- rating_scaler: double (nullable = true)\n",
      " |-- clean_review: string (nullable = true)\n",
      " |-- sentiment_encode: double (nullable = true)\n",
      " |-- rating_encode: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n",
      "There are 29958 rows and 8 columns in dataframe\n"
     ]
    }
   ],
   "source": [
    "# create label with fake news = 0\n",
    "review_data.show(10)\n",
    "review_data.printSchema()\n",
    "print(f'There are {review_data.count()} rows and {len(review_data.columns)} columns in dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a02ef3-c4fa-42b7-88ee-dfb7617326e2",
   "metadata": {},
   "source": [
    "- Combine 2 data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e00f4a18-2377-4a02-bb0c-c46f301ec167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 2204|\n",
      "|  1.0|16717|\n",
      "|  2.0|11037|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_data.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7600798-77d3-492d-929c-131150d1a602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------+------------------+--------------------+----------------+-------------+-----+\n",
      "|IDRestaurant|          date_time|user_id|     rating_scaler|        clean_review|sentiment_encode|rating_encode|label|\n",
      "+------------+-------------------+-------+------------------+--------------------+----------------+-------------+-----+\n",
      "|           1|2023-12-20 21:13:00|   8670| 5.680000000000001|gà chiên còn sống...|             0.0|          2.0|  2.0|\n",
      "|           1|2023-09-25 15:43:00|  11063|               5.5|     đã ăn ăn đồng_ý|             0.0|          2.0|  2.0|\n",
      "|           1|2023-06-24 11:11:00|   9541|               5.5|     đã thư rất ngon|             1.0|          2.0|  2.0|\n",
      "|           1|2022-12-22 14:58:00|   9112|               1.9|về gói thêm khách...|             0.0|          0.0|  0.0|\n",
      "|           1|2022-09-23 22:40:00|   9651|              4.78|nhỏ kêu đói hồi b...|             0.0|          2.0|  2.0|\n",
      "|           1|2022-09-15 11:32:00|  11343|              7.66|có khuyến_mại tặn...|             1.0|          1.0|  1.0|\n",
      "|           1|2022-07-04 01:31:00|    376|               1.9|thề lâu lắm mới t...|             0.0|          0.0|  0.0|\n",
      "|           1|2022-06-25 20:13:00|  13768|              10.0|là lần đầu bạn đế...|             0.0|          1.0|  2.0|\n",
      "|           1|2022-06-10 12:43:00|   3643|               1.9|đặt kết_hợp thêm ...|             2.0|          0.0|  0.0|\n",
      "|           1|2022-03-19 13:48:00|    446|              6.04|hích phần cơm dở ...|             0.0|          2.0|  2.0|\n",
      "|           1|2022-01-26 22:31:00|   9552|               1.9|            dởm cười|             1.0|          0.0|  2.0|\n",
      "|           1|2021-06-10 22:45:00|  12566|              4.78|tới lúc bạn nhân_...|             2.0|          2.0|  2.0|\n",
      "|           1|2021-05-19 14:57:00|   1736|               1.9|sáng đã ghé đakao...|             2.0|          0.0|  0.0|\n",
      "|           1|2021-04-24 19:37:00|   6788|4.0600000000000005|thiếu gà nhờ hàng...|             0.0|          2.0|  2.0|\n",
      "|           1|2021-01-13 17:00:00|  11386|              8.02|đi bạn dùng tại_c...|             0.0|          1.0|  2.0|\n",
      "|           1|2021-01-02 22:42:00|  12116|               7.3|                null|             2.0|          1.0|  1.0|\n",
      "|           1|2020-12-14 22:16:00|  13146|               1.9|ăn tại_chỗ không_...|             0.0|          0.0|  0.0|\n",
      "|           1|2020-12-14 00:14:00|   6983|              7.12|càng ngày càng ch...|             0.0|          1.0|  2.0|\n",
      "|           1|2020-12-11 13:48:00|  13146|               1.9|đặt phần miếng ứn...|             1.0|          0.0|  2.0|\n",
      "|           1|2020-10-29 22:49:00|   6377|               1.9|drive xuyên nhớ k...|             2.0|          0.0|  0.0|\n",
      "+------------+-------------------+-------+------------------+--------------------+----------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249204d-2dcf-4b5a-8d1f-c7d415285555",
   "metadata": {},
   "source": [
    "## check null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bc4183f-36d2-4004-ac0e-f0292ec41f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------+------------+----------------+-------------+-----+\n",
      "|IDRestaurant|user_id|rating_scaler|clean_review|sentiment_encode|rating_encode|label|\n",
      "+------------+-------+-------------+------------+----------------+-------------+-----+\n",
      "|           0|      0|            0|           0|               0|            0|    0|\n",
      "+------------+-------+-------------+------------+----------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_data.select([f.count(f.when(f.isnan(c), c)).alias(c) for c in review_data.columns if c not in ['date_time']]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f0bca37-365b-4c0f-ab4c-0852c71286cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------+------------+----------------+-------------+-----+\n",
      "|IDRestaurant|user_id|rating_scaler|clean_review|sentiment_encode|rating_encode|label|\n",
      "+------------+-------+-------------+------------+----------------+-------------+-----+\n",
      "|           0|      0|            0|         110|               0|            0|    0|\n",
      "+------------+-------+-------------+------------+----------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_data.select([f.count(f.when(f.isnull(c), c)).alias(c) for c in review_data.columns if c not in ['date_time']]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "033cb14e-9dc9-4274-a498-6eee3c9478f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------+-----------------+------------+----------------+-------------+-----+\n",
      "|IDRestaurant|          date_time|user_id|    rating_scaler|clean_review|sentiment_encode|rating_encode|label|\n",
      "+------------+-------------------+-------+-----------------+------------+----------------+-------------+-----+\n",
      "|           1|2021-01-02 22:42:00|  12116|              7.3|        null|             2.0|          1.0|  1.0|\n",
      "|          19|2023-11-22 13:07:00|   8334|              5.5|        null|             2.0|          2.0|  2.0|\n",
      "|          19|2023-11-22 10:44:00|   9208|             10.0|        null|             2.0|          1.0|  1.0|\n",
      "|          20|2024-01-25 10:55:00|   1438|             10.0|        null|             2.0|          1.0|  1.0|\n",
      "|          38|2023-10-10 13:06:00|  10272|              5.5|        null|             2.0|          2.0|  2.0|\n",
      "|          38|2023-10-09 13:02:00|  11209|             10.0|        null|             2.0|          1.0|  1.0|\n",
      "|          38|2023-10-09 13:02:00|   4777|             8.74|        null|             2.0|          1.0|  1.0|\n",
      "|          51|2024-01-25 10:55:00|   1438|             10.0|        null|             2.0|          1.0|  1.0|\n",
      "|          51|2023-11-21 10:49:00|   3953|              6.4|        null|             2.0|          2.0|  2.0|\n",
      "|          51|2023-11-21 10:46:00|   3953|              6.4|        null|             2.0|          2.0|  2.0|\n",
      "|          63|2018-01-24 16:39:00|    408|             7.84|        null|             2.0|          1.0|  1.0|\n",
      "|          66|2021-03-08 22:20:00|  11938|             8.74|        null|             2.0|          1.0|  1.0|\n",
      "|          67|2019-03-05 14:14:00|  13551|             10.0|        null|             2.0|          1.0|  1.0|\n",
      "|          83|2016-09-23 13:20:00|  14003|             8.02|        null|             2.0|          1.0|  1.0|\n",
      "|          91|2019-01-30 17:39:00|    235|              1.9|        null|             2.0|          0.0|  0.0|\n",
      "|          97|2018-03-23 15:27:00|  12953|             10.0|        null|             2.0|          1.0|  1.0|\n",
      "|         115|2023-09-05 19:23:00|   9772|6.760000000000001|        null|             2.0|          2.0|  2.0|\n",
      "|         125|2019-04-10 03:53:00|   2438|             3.52|        null|             2.0|          0.0|  0.0|\n",
      "|         132|2019-05-20 17:46:00|   9816|             10.0|        null|             2.0|          1.0|  1.0|\n",
      "|         135|2019-03-07 15:23:00|  10881|             10.0|        null|             2.0|          1.0|  1.0|\n",
      "+------------+-------------------+-------+-----------------+------------+----------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df contains null values => drop it\n",
    "review_data.filter(f.isnull(f.col('clean_review'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d03c1b-639b-4a02-afc8-070008fc66c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------------+--------------------+----------------+-------------+-----+\n",
      "|IDRestaurant|user_id|    rating_scaler|        clean_review|sentiment_encode|rating_encode|label|\n",
      "+------------+-------+-----------------+--------------------+----------------+-------------+-----+\n",
      "|           1|   8670|5.680000000000001|gà chiên còn sống...|             0.0|          2.0|  2.0|\n",
      "|           1|  11063|              5.5|     đã ăn ăn đồng_ý|             0.0|          2.0|  2.0|\n",
      "|           1|   9541|              5.5|     đã thư rất ngon|             1.0|          2.0|  2.0|\n",
      "|           1|   9112|              1.9|về gói thêm khách...|             0.0|          0.0|  0.0|\n",
      "|           1|   9651|             4.78|nhỏ kêu đói hồi b...|             0.0|          2.0|  2.0|\n",
      "|           1|  11343|             7.66|có khuyến_mại tặn...|             1.0|          1.0|  1.0|\n",
      "|           1|    376|              1.9|thề lâu lắm mới t...|             0.0|          0.0|  0.0|\n",
      "|           1|  13768|             10.0|là lần đầu bạn đế...|             0.0|          1.0|  2.0|\n",
      "|           1|   3643|              1.9|đặt kết_hợp thêm ...|             2.0|          0.0|  0.0|\n",
      "|           1|    446|             6.04|hích phần cơm dở ...|             0.0|          2.0|  2.0|\n",
      "+------------+-------+-----------------+--------------------+----------------+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- IDRestaurant: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- rating_scaler: double (nullable = true)\n",
      " |-- clean_review: string (nullable = true)\n",
      " |-- sentiment_encode: double (nullable = true)\n",
      " |-- rating_encode: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n",
      "There are 29848 rows and 7 columns in dataframe\n"
     ]
    }
   ],
   "source": [
    "clean_review_data = review_data.dropna(subset=['clean_review']).drop('date_time')\n",
    "clean_review_data.show(10)\n",
    "clean_review_data.printSchema()\n",
    "print(f'There are {clean_review_data.count()} rows and {len(clean_review_data.columns)} columns in dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960ced7-cd56-40b8-a44c-baf0e9999c30",
   "metadata": {},
   "source": [
    "## Descirble data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75352a4a-dc69-4c57-a089-939a93329f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subject']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_col = ['subject']\n",
    "cat_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f070eef-dbb7-4a12-8f88-b93b60c3eca1",
   "metadata": {},
   "source": [
    "## Category data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c5bd0-1dd2-4a5c-982e-d637862161c7",
   "metadata": {},
   "source": [
    "### Check distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d165718f-19b3-4640-be2d-d86eae99253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----------+\n",
      "|IDRestaurant|label|Count label|\n",
      "+------------+-----+-----------+\n",
      "|        1589|  1.0|         96|\n",
      "|         475|  1.0|         94|\n",
      "|         917|  2.0|         94|\n",
      "|        1180|  1.0|         92|\n",
      "|        1164|  1.0|         90|\n",
      "|        1521|  1.0|         90|\n",
      "|         161|  1.0|         89|\n",
      "|        1277|  1.0|         86|\n",
      "|        1169|  1.0|         86|\n",
      "|         523|  1.0|         85|\n",
      "|         772|  1.0|         83|\n",
      "|          20|  1.0|         81|\n",
      "|         543|  1.0|         81|\n",
      "|         600|  1.0|         81|\n",
      "|         169|  1.0|         79|\n",
      "|         371|  1.0|         78|\n",
      "|          51|  1.0|         78|\n",
      "|         198|  1.0|         78|\n",
      "|         603|  1.0|         77|\n",
      "|        1074|  1.0|         77|\n",
      "+------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(clean_review_data\n",
    " .groupby('IDRestaurant','label')\n",
    " .agg(f.count('rating_encode').alias('Count label'))\n",
    " .orderBy(f.col('Count label').desc())\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679ab99-cedb-4248-8547-0e292df8c995",
   "metadata": {},
   "source": [
    "### Check value count of column label in df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae8443e6-b404-4394-830f-f986c40d5c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------------+\n",
      "|label|count|          Normalize|\n",
      "+-----+-----+-------------------+\n",
      "|  1.0|16648| 0.5577593138568748|\n",
      "|  2.0|11010| 0.3688689359421067|\n",
      "|  0.0| 2190|0.07337175020101849|\n",
      "+-----+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(clean_review_data.groupby('label').count()\n",
    " .withColumn('Normalize',(f.col('count')/clean_review_data.count()))\n",
    " .orderBy(f.col('count').desc())\n",
    " .show(20,vertical=False,truncate=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67360951-646b-4acb-83f1-c28b8d4e5db9",
   "metadata": {
    "tags": []
   },
   "source": [
    "transform tf-idf of feature `clean_review`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f42901f-e789-46b6-b6dc-f64bcfc8a192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        clean_review|label|\n",
      "+--------------------+-----+\n",
      "|gà chiên còn sống...|  2.0|\n",
      "|     đã ăn ăn đồng_ý|  2.0|\n",
      "|     đã thư rất ngon|  2.0|\n",
      "|về gói thêm khách...|  0.0|\n",
      "|nhỏ kêu đói hồi b...|  2.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = clean_review_data.select('clean_review','label')\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcceb82-318a-49c0-a62f-52e62159025a",
   "metadata": {},
   "source": [
    "# Feature Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a887d51-074a-42f7-8af8-4b7aafa74719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#some parameter for countvectorizer\n",
    "#define vocabsize for maximum number of words list, the higher values is higher accuracy but more expensive computing.\n",
    "#define mindf for the minimum of the word frequency that appear in all dataset,\n",
    "#the word that appear less frequently than this threshold will be ignored\n",
    "_vocabsize = 1000\n",
    "_mindf = int((0.4/100)*(df_final.count()))\n",
    "_mindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34437a32-a0b9-4019-9832-177e7e20f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='clean_review',outputCol='token_text')\n",
    "remover = StopWordsRemover(inputCol='token_text', outputCol='stop_token')\n",
    "count_vec = CountVectorizer(inputCol='stop_token',outputCol='c_vec',vocabSize=_vocabsize, minDF=_mindf)\n",
    "idf = IDF(inputCol='c_vec', outputCol='tf_idf',minDocFreq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "998b85a5-665e-4965-87f9-7bcfb2df60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols=['tf_idf'],\n",
    "                           handleInvalid='keep',\n",
    "                           outputCol='features') # 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331c852-b5cb-432a-848c-07d7fb11fc8b",
   "metadata": {},
   "source": [
    "# Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dfaed57-6be3-45fd-88c5-0b70a8c29101",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pre_pipeline = Pipeline(stages=[tokenizer,\n",
    "                                   remover, \n",
    "                                   count_vec, \n",
    "                                   idf,\n",
    "                                   clean_up]).fit(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd08d1ac-c266-41f2-ac4b-212c0b83403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data = nlp_pre_pipeline.transform(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4afdbac7-6197-4a6c-8a06-ddeafa49b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_clean_data = trans_data.select('features','label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641670a4-2554-4897-8bac-3d8278493198",
   "metadata": {},
   "source": [
    "## Suffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09be6555-18f3-46a6-8959-4970e8399bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(978,[0,1,2,3,4,5...|  1.0|\n",
      "|(978,[0,1,4,7,12,...|  1.0|\n",
      "|(978,[2,6,8,17,21...|  1.0|\n",
      "|(978,[0,1,2,4,5,6...|  1.0|\n",
      "|(978,[0,9,13,15,3...|  1.0|\n",
      "|(978,[14,18,55,11...|  2.0|\n",
      "|(978,[0,1,3,4,5,7...|  1.0|\n",
      "|(978,[4,23,36,59,...|  1.0|\n",
      "|(978,[0,4,7,21,39...|  1.0|\n",
      "|(978,[1,2,3,4,5,7...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans_clean_data = trans_clean_data.orderBy(f.rand(42))\n",
    "trans_clean_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b350fea5-81a8-4186-be9c-f8491a6c466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = trans_clean_data.randomSplit([0.8,0.2],seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58d8bb3b-3aa7-4ca9-b8ed-9122752d3f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5842"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f8ff1-99c1-4749-9f1c-0ca00d6a58b0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2418fd-96f2-4006-baf0-1d73795eb166",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "77929459-c4d9-4722-a271-6076fa2ba1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to train model: 0.1582290291786194 mins\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nb_model = NaiveBayes(featuresCol='features',\n",
    "                    labelCol='label',\n",
    "                    predictionCol='prediction').fit(train)\n",
    "period = (time.time() - start)/60\n",
    "print(f'Total time to train model: {period} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52117314-41ab-4605-9f3f-9b12ce9c8824",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(stages=[nlp_pre_pipeline,nb_model]).fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac7b24-b19b-490d-8263-8307b92ea5a9",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b69efe03-ad44-4f30-ab9f-e456b0220a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to train model: 0.3419677257537842 mins\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rf = RandomForestClassifier(featuresCol='features',\n",
    "                            labelCol='label',\n",
    "                            predictionCol='prediction').fit(train)\n",
    "period = (time.time() - start)/60\n",
    "print(f'Total time to train model: {period} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31fe61-c743-4dfa-85c0-26bb708e6b85",
   "metadata": {},
   "source": [
    "## Multilayer perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ed173e0-7ce1-4621-8ba5-f03780920778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to train model: 0.4014607350031535 mins\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "layers = [5, 5, 4, 3]\n",
    "pecep = MultilayerPerceptronClassifier(maxIter=100, \n",
    "                                       layers=layers, \n",
    "                                       blockSize=128, \n",
    "                                       seed=1234)\n",
    "pecep_model = pecep.fit(train)\n",
    "period = (time.time() - start)/60\n",
    "print(f'Total time to train model: {period} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2a8a5-f1d0-4458-b61e-2bdbe15a61f9",
   "metadata": {},
   "source": [
    "**As can be seen, the fastest model is Naive Baiyes, Next model is Random Forest  and the last one is  Multilayer perceptron classifier** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bfc9bd-3ac2-4a20-acfc-967664d1d844",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c495648-241b-4b6e-bfec-e0112301dd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0|  375|\n",
      "|  1.0|       1.0| 2716|\n",
      "|  0.0|       1.0|   14|\n",
      "|  2.0|       2.0|  980|\n",
      "|  1.0|       0.0|   79|\n",
      "|  2.0|       1.0|  800|\n",
      "|  1.0|       2.0|  458|\n",
      "|  0.0|       0.0|  313|\n",
      "|  0.0|       2.0|  107|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_pred = nb_model.transform(test)\n",
    "nb_pred.groupby('label','prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11f5a27f-518f-47cd-b76f-cb75b6ec3e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0| 3246|\n",
      "|  0.0|       1.0|  329|\n",
      "|  2.0|       2.0|   81|\n",
      "|  2.0|       1.0| 2074|\n",
      "|  0.0|       2.0|  104|\n",
      "|  1.0|       2.0|    7|\n",
      "|  0.0|       0.0|    1|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_pred = rf.transform(test)\n",
    "rf_pred.groupby('label','prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fbccf9-c635-43b6-9580-26754d8dd64e",
   "metadata": {},
   "source": [
    "## Confusion matrix of 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e983abe9-4cfc-4b18-ab16-d34374f0fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Naive Bayes\n",
    "cm_nb = calculate_confusion_matrix(nb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d10651a0-b8d2-4067-8d99-a0cd72837098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 313,   14,  107],\n",
       "       [  79, 2716,  458],\n",
       "       [ 375,  800,  980]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Dacm_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a33d678c-38c6-43d3-ae95-55bdbb9bf207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Random Forest\n",
    "cm_rf = calculate_confusion_matrix(rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06e18f16-d575-47d1-a120-1dd7c3c0926d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  329,  104],\n",
       "       [   0, 3246,    7],\n",
       "       [   0, 2074,   81]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "66dcca72-9ab9-4ae2-9efe-8598a3282a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5184868195823348,\n",
       " 'recall': array([0.72119816, 0.83492161, 0.45475638]),\n",
       " 'precision': array([0.40808344, 0.7694051 , 0.63430421]),\n",
       " 'f1': array([0.52123231, 0.80082559, 0.52972973])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(cm_nb,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c4c98a2a-a101-41ac-b44f-80cc2fe867b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5558028072577884,\n",
       " 'recall': array([0.00230415, 0.99784814, 0.03758701]),\n",
       " 'precision': array([1.        , 0.57461498, 0.421875  ]),\n",
       " 'f1': array([0.0045977 , 0.72927432, 0.06902429])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(cm_rf,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca5a5e-91a8-4be3-aac6-56864c7e2f03",
   "metadata": {},
   "source": [
    "- Random Forest give the worst result than Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27338c8a-b9e7-45c0-8c35-991398c04842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of Random Forest : 0.5696679219445395\n",
      "Accuracy score of Naive Bayes: 0.6862375898664841\n",
      "\n",
      "\n",
      "Precision score of Random Forest : 0.5498738689249729\n",
      "Precision score of Naive Bayes: 0.6927265607152568\n",
      "\n",
      "\n",
      "Recall score of Random Forest : 0.5696679219445396\n",
      "Recall score of Naive Bayes: 0.6862375898664841\n",
      "\n",
      "\n",
      "F1 score of Random Forest : 0.43188498861907987\n",
      "F1 score of Naive Bayes: 0.6800527291791921\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_evaluator = MulticlassClassificationEvaluator(predictionCol='prediction',labelCol='label')\n",
    "\n",
    "# Naive Baiyes\n",
    "acc_nb = multi_evaluator.evaluate(nb_pred,{multi_evaluator.metricName:'accuracy'})\n",
    "precision_nb = multi_evaluator.evaluate(nb_pred,{multi_evaluator.metricName:'weightedPrecision'})\n",
    "recall_nb = multi_evaluator.evaluate(nb_pred,{multi_evaluator.metricName:'weightedRecall'})\n",
    "f1_nb = multi_evaluator.evaluate(nb_pred,{multi_evaluator.metricName:'f1'})\n",
    "\n",
    "# Random Forest \n",
    "acc_rf = multi_evaluator.evaluate(rf_pred,{multi_evaluator.metricName:'accuracy'})\n",
    "precision_rf = multi_evaluator.evaluate(rf_pred,{multi_evaluator.metricName:'weightedPrecision'})\n",
    "recall_rf = multi_evaluator.evaluate(rf_pred,{multi_evaluator.metricName:'weightedRecall'})\n",
    "f1_rf = multi_evaluator.evaluate(rf_pred,{multi_evaluator.metricName:'f1'})\n",
    "\n",
    "\n",
    "print(\"Accuracy score of Random Forest :\", acc_rf)\n",
    "print(\"Accuracy score of Naive Bayes:\", acc_nb)\n",
    "print('\\n')\n",
    "print(\"Precision score of Random Forest :\", precision_rf)\n",
    "print(\"Precision score of Naive Bayes:\", precision_nb)\n",
    "print('\\n')\n",
    "print(\"Recall score of Random Forest :\", recall_rf)\n",
    "print(\"Recall score of Naive Bayes:\", recall_nb)\n",
    "print('\\n')\n",
    "print(\"F1 score of Random Forest :\", f1_rf)\n",
    "print(\"F1 score of Naive Bayes:\", f1_nb)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff2112-b560-4a0e-b274-82683503dcf7",
   "metadata": {},
   "source": [
    "**From the above results, we can see Naive Baiyes better than Random Forest in both performance and training time**\n",
    "\n",
    "`=> Choose Naive Bayes`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b58324-c8a9-4cbf-b0a7-09a87bb26347",
   "metadata": {},
   "source": [
    "# Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4747ed78-05a8-468e-97eb-65e50ee25b2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1849.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.classification.NaiveBayesModel$NaiveBayesModelWriter.saveImpl(NaiveBayes.scala:571)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\t... 51 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnb_classifier\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\ml\\util.py:197\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mD:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\PySpark\\spark-3.3.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1849.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.classification.NaiveBayesModel$NaiveBayesModelWriter.saveImpl(NaiveBayes.scala:571)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\t... 51 more\r\n"
     ]
    }
   ],
   "source": [
    "nb_model.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
