{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7d2b4-9253-4a91-bce8-7cb38943acbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae0cb902-d5e3-447a-8063-bb0c2d0d5316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Project1.Code.project_transformer import Data_Wrangling\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Project1.Code.utilities import display_emotion, get_top_duplicated_words, create_adj_wordcloud, restaurant_sentiment_analysis, get_dict_word #preprocessing_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fafbcef-b5a8-4e5b-90b7-5fdbf5a4ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text,\n",
    "                       cleanser,\n",
    "                       vector_model,  # ='resources/countvectorizer_model.pkl',\n",
    "                       tfidf_model):  # 'resources/tfidf_model.pkl'\n",
    "    #clean_text = cleanser.process_text(text=text)\n",
    "    #clean_text = cleanser.convert_unicode(str(clean_text))\n",
    "    clean_text = cleanser.process_postag_thesea(text)\n",
    "    print(clean_text)\n",
    "    # initialize countvectorizer and TFIDF\n",
    "    vectorizer = vector_model\n",
    "    tfidf = tfidf_model\n",
    "\n",
    "    # transform X_test\n",
    "    _pre_X_test = vectorizer.transform([clean_text])\n",
    "    _pre_X_test = tfidf.transform(_pre_X_test)\n",
    "\n",
    "    return _pre_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48d1ffaa-9785-4da1-8d22-4b92bba35899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\App_data\\anaconda3\\envs\\env39\\lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator CountVectorizer from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\App_data\\anaconda3\\envs\\env39\\lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ----------------- INITIALIZATION -----------------\n",
    "emoji_dict = get_dict_word(file_path='Project1/DATA_shopeefood/files/emojicon.txt')  # Emoji\n",
    "teen_dict = get_dict_word(file_path='Project1/DATA_shopeefood/files/teencode.txt')  # Teen\n",
    "eng_dict = get_dict_word(file_path='Project1/DATA_shopeefood/files/english-vnmese.txt')  # eng\n",
    "\n",
    "with open('Project1/DATA_shopeefood/files/wrong-word.txt', 'r', encoding=\"utf8\") as file:  # WRONG WORD\n",
    "    wrong_lst = file.read().split('\\n')\n",
    "\n",
    "with open('Project1/DATA_shopeefood/files/vietnamese-stopwords.txt', 'r', encoding=\"utf8\") as file:  # STOP WORD\n",
    "    stop_lst = file.read().split('\\n')\n",
    "\n",
    "# GET CLEANSER\n",
    "cleanser = Data_Wrangling(emoji_dict=emoji_dict,\n",
    "                          teen_dict=teen_dict,\n",
    "                          wrong_lst=wrong_lst,\n",
    "                          eng_vn_dict=eng_dict,\n",
    "                          stop_words=stop_lst)\n",
    "# Model Vectorize and TFIDF\n",
    "with open('resources/countvectorizer_model.pkl', 'rb') as f:\n",
    "    vec_m = pickle.load(f)\n",
    "with open('resources/tfidf_model.pkl', 'rb') as f_2:\n",
    "    tfidf_m = pickle.load(f_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b975ad31-162a-4f57-a640-1e0ec043f622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quán này ngon quá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Đồ ăn dở tệ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ăn cũng được</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Content\n",
       "0  Quán này ngon quá\n",
       "1        Đồ ăn dở tệ\n",
       "2       Ăn cũng được"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cee9098c-3e7f-4e64-8b88-826240175045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ggg   (0, 10011)\t1.0\n",
      "1 ggg   (0, 14988)\t1.0\n",
      "2 ggg \n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(X_pre):\n",
    "    print(i,'ggg',v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5004ad2-1664-4bb3-b9bc-f834f361c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\tComment\n",
    "1\tGà chiên còn sống, rất tanh. Khủng khiép\n",
    "2\tĐã ăn qua rồi, ăn ok ạ\n",
    "3\tĐã thư rất ngon\n",
    "4\tTệ .. mua mang về cho 1 gói tương ớt. Và xin thêm không được. Khách ơ lại ăn thì tương free. Mua về thì lại phải mua thêm tương. Nhưng phần ăn 3 món. Cho 1 gói. Nvien không linh hoạt. Chuỗi lớn, còn thua cả một tiệm ăn nhỏ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd00cacf-5f8d-4f36-8799-d4252d755406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10011)\t1.0\n",
      "[2]\n",
      "  (0, 14988)\t1.0\n",
      "[2]\n",
      "\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "for x in X_pre:\n",
    "    print(x)\n",
    "    re = model.predict(x)\n",
    "    print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16db2fe6-9a37-4653-b83b-7d19ef127bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đồ_ăn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = preprocessing_text(text=lines_df['Content'][1],\n",
    "                       cleanser=cleanser,\n",
    "                       vector_model=vec_m,\n",
    "                       tfidf_model=tfidf_m)\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1acebd-ccfc-4f5d-9b4f-c5d56bb233bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\App_data\\anaconda3\\envs\\env39\\lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'csr_matrix'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      5\u001b[0m lines_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(uploaded_file_1, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m X_pre \u001b[38;5;241m=\u001b[39m lines_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocessing_text(text\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m      8\u001b[0m                                                                cleanser\u001b[38;5;241m=\u001b[39mcleanser,\n\u001b[0;32m      9\u001b[0m                                                                vector_model\u001b[38;5;241m=\u001b[39mvec_m,\n\u001b[0;32m     10\u001b[0m                                                                tfidf_model\u001b[38;5;241m=\u001b[39mtfidf_m))\n\u001b[1;32m---> 12\u001b[0m prediction_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_pre\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#print(prediction_)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m lines_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m prediction_\n",
      "File \u001b[1;32mD:\\App_data\\anaconda3\\envs\\env39\\lib\\site-packages\\sklearn\\svm\\_base.py:818\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    816\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 818\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[1;32mD:\\App_data\\anaconda3\\envs\\env39\\lib\\site-packages\\sklearn\\svm\\_base.py:431\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 431\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32mD:\\App_data\\anaconda3\\envs\\env39\\lib\\site-packages\\sklearn\\svm\\_base.py:611\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    608\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[1;32m--> 611\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39misspmatrix(X):\n\u001b[0;32m    621\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32mD:\\App_data\\anaconda3\\envs\\env39\\lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mD:\\App_data\\anaconda3\\envs\\env39\\lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mD:\\App_data\\anaconda3\\envs\\env39\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "pkl_filename = \"resources/project_1_model_SVC.sav\"\n",
    "model = pickle.load(open(pkl_filename, 'rb'))\n",
    "\n",
    "uploaded_file_1 = 'resources/sample/Content.txt'\n",
    "lines_df = pd.read_csv(uploaded_file_1, header=0)\n",
    "\n",
    "X_pre = lines_df['Content'].apply(lambda x: preprocessing_text(text=x,\n",
    "                                                               cleanser=cleanser,\n",
    "                                                               vector_model=vec_m,\n",
    "                                                               tfidf_model=tfidf_m))\n",
    "\n",
    "prediction_ = model.predict(np.array([X_pre]))\n",
    "#print(prediction_)\n",
    "lines_df['Prediction'] = prediction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b0ba4-8472-4e48-81d2-d07b9c66dae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47055ec-824c-49b0-a099-2eb6f4a3e6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a057c4-dccd-42eb-bb3c-8630c8006297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
